# Agent Conclave — example configuration
# Get a second AI opinion without being human middleware.
#
# Configure two or more models, assign roles, and let them debate.
# White smoke when they agree, black smoke when they don't.

models:
  cards:
    # Local models via Ollama (free, no API key)
    - id: cli:llama
      command: [ollama, run, llama3.2]
      prompt_mode: arg
      timeout_seconds: 120

    - id: cli:qwen
      command: [ollama, run, qwen2.5:7b]
      prompt_mode: arg
      timeout_seconds: 120

    # Uncomment to use hosted models instead:
    # - id: cli:claude
    #   command: [claude, --print, --model, sonnet, --output-format, text]
    #   prompt_mode: arg
    #   timeout_seconds: 300
    # - id: cli:codex
    #   command: [codex, exec, --ephemeral, --sandbox, read-only]
    #   prompt_mode: stdin
    #   stdin_flag: "-"
    #   timeout_seconds: 300

# Assign roles — who argues FOR, who argues AGAINST
planner:
  role_overrides:
    reasoner: cli:llama       # builds the case
    critic: cli:qwen          # tears it apart
    summarizer: cli:qwen      # synthesizes the verdict

# Deliberation settings
deliberation:
  max_rounds: 5               # max back-and-forth rounds
  stability_rounds: 2         # stop after 2 consecutive agreements
  model_timeout_seconds: 120  # per-model call timeout

  # Panel: optional tiebreaker when reasoner and critic disagree
  panel:
    enabled: false
    # model_ids: [cli:gemini]
    # include_plan_models: false
    # max_rounds: 1

# Disable extras for simple setup
annealing:
  enabled: false

diversity_check:
  enabled: false
